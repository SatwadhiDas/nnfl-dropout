{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatwadhiDas/nnfl-dropout/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E49TZc1HfzTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import decimal\n",
        "import os\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlDu3ahLjFfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOl9AGeOjG6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def findValueN(prob):\n",
        "    # pn = 256 for 2 first hidden layers\n",
        "    # pn = 512 for last layer\n",
        "        # n values are the number of hidden units at each layer\n",
        "    n1 = int(256.0/prob)\n",
        "    n2 = int(256.0/prob)\n",
        "    n3 = int(512.0/prob)\n",
        "    n = [n1,n2,n3,prob]\n",
        "        # return number o\n",
        "    float(n[3])\n",
        "    return n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXoA46p7jMfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addPValue(prob):\n",
        "        # layers all have 2048 hidden units as described in the research paper\n",
        "    n1 = 2048\n",
        "    n2 = 2048\n",
        "    n3 = 2048\n",
        "    n = [n1,n2,n3,prob]\n",
        "    float(n[3])\n",
        "    return n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWO61_hajXpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def runDropout(*layer):\n",
        "        # Sequential model and the layers that describe the model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape, activation=\"relu\"))\n",
        "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "    model.add(Dense(layer[0], activation='relu'))\n",
        "    model.add(Dropout(layer[3]))\n",
        "    model.add(Dense(layer[1], activation='relu'))\n",
        "    model.add(Dropout(layer[3]))\n",
        "    model.add(Dense(layer[2], activation='relu'))\n",
        "    model.add(Dropout(layer[3]))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "        # Configure model before training\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    print(x_train.shape[0], 'train samples')\n",
        "    print(x_test.shape[0], 'test samples')\n",
        "    batch_size = 128\n",
        "    epochs = 10\n",
        "\n",
        "        # Train the model for a fixed number of epochs\n",
        "    history_dropout = model.fit_generator(training_set, \n",
        "                         steps_per_epoch=60000//64, \n",
        "                         validation_data= test_set, \n",
        "                         validation_steps=10000//64, \n",
        "                         epochs=10)\n",
        "    \n",
        "    y_pred=model.predict_classes(x_test);\n",
        "    y_test_res = y_test.reshape(1,y_test.shape[0],1);\n",
        "    y_pred_res = y_pred.reshape(1,y_test.shape[0],1);\n",
        "    print(tf.image.psnr(y_test_res,y_pred_res,max_val=1.0),'PSNR VALUE')\n",
        "\n",
        "        # Training accuracy\n",
        "    accuracy = history_dropout.history['accuracy']\n",
        "    train_err = 100.0 - 100.0*(accuracy[-1])\n",
        "\n",
        "        # Test Accuracy\n",
        "            # Validation accuracy possibly not same as Test accuracy\n",
        "    val_acc = history_dropout.history['val_accuracy']\n",
        "    test_err = 100.0 - 100.0*(val_acc[-1])\n",
        "    # *** Calculate error bars HERE ***\n",
        "    finalError = [test_err, train_err, layer[3]]\n",
        "    return finalError\n",
        "    # END OF RUN DROPOUT #\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwoF5QCRjg25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def floatRange(start, stop, step):\n",
        "  while start <= stop:\n",
        "    yield float(start)\n",
        "    start += decimal.Decimal(step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvcnWM5Qjpcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def runForAllP():\n",
        "    # lists to store ploting values locally\n",
        "        # figure a error\n",
        "    a_test_error = []\n",
        "    a_train_error = []\n",
        "        # figure b error\n",
        "    b_test_error = []\n",
        "    b_train_error = []\n",
        "        # returns the list of p values from 0.1-1.0, into list\n",
        "    pValues = [0.1,0.2]\n",
        "    pValues1 = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "\n",
        "    print(pValues)\n",
        "    # run dropout on all values of p for both figures\n",
        "    for i in pValues:\n",
        "        varyLayer = findValueN(i)\n",
        "        constLayer = addPValue(i)\n",
        "            # Store error values for figure a\n",
        "        errorFigA = runDropout(*constLayer)\n",
        "        a_test_error.append(errorFigA[0])\n",
        "        a_train_error.append(errorFigA[1])\n",
        "        print(\"a_train_error\")\n",
        "        print(a_train_error)\n",
        "        print(\"\\n\")\n",
        "        print(\"a_test_error\")\n",
        "        print(a_test_error)\n",
        "        print(\"\\n\")\n",
        "    for i in pValues:\n",
        "        varyLayer = findValueN(i)\n",
        "        constLayer = addPValue(i)\n",
        "        errorFigB = runDropout(*varyLayer)\n",
        "        # Store error values for figure b\n",
        "        b_test_error.append(errorFigB[0])\n",
        "        b_train_error.append(errorFigB[1])\n",
        "        print(\"b_train_error\")\n",
        "        print(b_train_error)\n",
        "        print(\"\\n\")\n",
        "        print(\"b_test_error\")\n",
        "        print(b_test_error)\n",
        "        print(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqQn4QfNj6IS",
        "colab_type": "code",
        "outputId": "f2d317b2-7e19-487e-b605-51002240bdb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) =mnist.load_data()\n",
        "\n",
        "# Reshaping the array to 4-dims so that it can work with the API\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "# Set values to float so that we can get decimal points after division\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "# Normalizing the RGB codes by dividing it to the max RGB value\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "train_gen = ImageDataGenerator(rotation_range=8, \n",
        "                               width_shift_range=0.08, \n",
        "                               shear_range=0.3, \n",
        "                               height_shift_range=0.08, \n",
        "                               zoom_range=0.08 )\n",
        "test_gen = ImageDataGenerator()\n",
        "\n",
        "training_set= train_gen.flow(x_train, y_train, batch_size=64)\n",
        "test_set= train_gen.flow(x_test, y_test, batch_size=64)\n",
        "\n",
        "# runs both archetectures for all values of p (from 0.1-1.0, inclusive)\n",
        "# Currently takes too long to run, but I'm confident that the output will come\n",
        "    # close to the results in the figure\n",
        "runForAllP()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "[0.1, 0.2]\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/10\n",
            "937/937 [==============================] - 1311s 1s/step - loss: 0.2547 - accuracy: 0.9213 - val_loss: 0.1405 - val_accuracy: 0.9627\n",
            "Epoch 2/10\n",
            "937/937 [==============================] - 1296s 1s/step - loss: 0.1193 - accuracy: 0.9656 - val_loss: 0.1273 - val_accuracy: 0.9731\n",
            "Epoch 3/10\n",
            "937/937 [==============================] - 1296s 1s/step - loss: 0.0958 - accuracy: 0.9729 - val_loss: 0.1088 - val_accuracy: 0.9750\n",
            "Epoch 4/10\n",
            "937/937 [==============================] - 1297s 1s/step - loss: 0.0852 - accuracy: 0.9759 - val_loss: 0.0473 - val_accuracy: 0.9802\n",
            "Epoch 5/10\n",
            "937/937 [==============================] - 1297s 1s/step - loss: 0.0748 - accuracy: 0.9778 - val_loss: 0.0973 - val_accuracy: 0.9795\n",
            "Epoch 6/10\n",
            "937/937 [==============================] - 1296s 1s/step - loss: 0.0690 - accuracy: 0.9803 - val_loss: 0.0247 - val_accuracy: 0.9772\n",
            "Epoch 7/10\n",
            "937/937 [==============================] - 1304s 1s/step - loss: 0.0648 - accuracy: 0.9819 - val_loss: 0.0280 - val_accuracy: 0.9807\n",
            "Epoch 8/10\n",
            "937/937 [==============================] - 1318s 1s/step - loss: 0.0608 - accuracy: 0.9824 - val_loss: 0.0532 - val_accuracy: 0.9815\n",
            "Epoch 9/10\n",
            "937/937 [==============================] - 1303s 1s/step - loss: 0.0594 - accuracy: 0.9832 - val_loss: 0.0031 - val_accuracy: 0.9825\n",
            "Epoch 10/10\n",
            "937/937 [==============================] - 1307s 1s/step - loss: 0.0539 - accuracy: 0.9846 - val_loss: 0.1395 - val_accuracy: 0.9842\n",
            "tf.Tensor(-14.491539, shape=(), dtype=float32) PSNR VALUE\n",
            "b_train_error\n",
            "[1.5416443347930908]\n",
            "\n",
            "\n",
            "b_test_error\n",
            "[1.580113172531128]\n",
            "\n",
            "\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/10\n",
            "937/937 [==============================] - 576s 615ms/step - loss: 0.2564 - accuracy: 0.9189 - val_loss: 0.2496 - val_accuracy: 0.9641\n",
            "Epoch 2/10\n",
            "937/937 [==============================] - 574s 612ms/step - loss: 0.1173 - accuracy: 0.9654 - val_loss: 0.2574 - val_accuracy: 0.9742\n",
            "Epoch 3/10\n",
            "937/937 [==============================] - 571s 609ms/step - loss: 0.0966 - accuracy: 0.9725 - val_loss: 0.0265 - val_accuracy: 0.9806\n",
            "Epoch 4/10\n",
            "937/937 [==============================] - 569s 607ms/step - loss: 0.0857 - accuracy: 0.9751 - val_loss: 0.0443 - val_accuracy: 0.9804\n",
            "Epoch 5/10\n",
            "937/937 [==============================] - 571s 609ms/step - loss: 0.0771 - accuracy: 0.9785 - val_loss: 0.0759 - val_accuracy: 0.9799\n",
            "Epoch 6/10\n",
            "937/937 [==============================] - 569s 607ms/step - loss: 0.0717 - accuracy: 0.9792 - val_loss: 0.0479 - val_accuracy: 0.9829\n",
            "Epoch 7/10\n",
            "937/937 [==============================] - 566s 604ms/step - loss: 0.0685 - accuracy: 0.9803 - val_loss: 0.0104 - val_accuracy: 0.9804\n",
            "Epoch 8/10\n",
            "937/937 [==============================] - 568s 606ms/step - loss: 0.0650 - accuracy: 0.9818 - val_loss: 0.1362 - val_accuracy: 0.9835\n",
            "Epoch 9/10\n",
            "937/937 [==============================] - 568s 606ms/step - loss: 0.0622 - accuracy: 0.9819 - val_loss: 0.1923 - val_accuracy: 0.9830\n",
            "Epoch 10/10\n",
            "937/937 [==============================] - 572s 610ms/step - loss: 0.0579 - accuracy: 0.9834 - val_loss: 0.0101 - val_accuracy: 0.9848\n",
            "tf.Tensor(-14.491539, shape=(), dtype=float32) PSNR VALUE\n",
            "b_train_error\n",
            "[1.5416443347930908, 1.6567647457122803]\n",
            "\n",
            "\n",
            "b_test_error\n",
            "[1.580113172531128, 1.5197277069091797]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0d9044ea0279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Currently takes too long to run, but I'm confident that the output will come\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# close to the results in the figure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mrunForAllP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-00f30539bf8c>\u001b[0m in \u001b[0;36mrunForAllP\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Probability of retaining a unit (p)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpValues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_test_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Test Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpValues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_train_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \"\"\"\n\u001b[1;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (2,) and (0,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
